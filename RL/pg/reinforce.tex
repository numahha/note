\section{REINFORCE \cite{williams1992simple}}
軌道$\tau=(s_0,a_0,s_1,a_1,\cdots,s_T)$は，終端状態にたどり着くまで，方策$a_t\sim \pi(a_t|s_t)$に従って行動をサンプリングし，またダイナミクス$s_{t+1}\sim P(s_{t+1}|s_t,a_t)$に従って状態をサンプリングすることにより生成される．
報酬$r(s_t,a_t)$を，各時間ステップにおいて受け取る．
確率的定常方策$\pi$で表し，$a_t\sim\pi(a_t|s_t)$とする．
$\pi$を特徴づけるパラメータを$\theta$とする．

軌道$\tau$が得られる確率は，次のように書ける．
\begin{align*}
p(\tau) &= p(s_0)\prod_{t=0}^{T-1}\pi(a_t|s_t)p(s_{t+1}|s_t,a_t)
\\
\ln p(\tau) &= \ln p(s_0)+\sum_{t=0}^{T-1}\left(\ln\pi(a_t|s_t)+\ln p(s_{t+1}|s_t,a_t)\right)
\end{align*}
これを微分すると，
\begin{align*}
\nabla_\theta p(\tau)
=
p(\tau)\nabla_\theta \ln p(\tau)
=
p(\tau)\left(
\sum_{t=0}^{T-1}\nabla_\theta \ln\pi(a_t|s_t)\right)
\end{align*}
報酬和を$R(\tau)$とすると，
\begin{align*}
E[R(\tau)] &= \sum_{\tau}p(\tau)R(\tau)
\\
\nabla_\theta E[R(\tau)] &= \nabla_\theta\sum_{\tau}p(\tau)R(\tau)
= \sum_{\tau}R(\tau) \nabla_\theta p(\tau)
= \sum_{\tau}R(\tau) p(\tau)
\left(\sum_{t=0}^{T-1}\nabla_\theta \ln\pi(a_t|s_t)\right)
\\
 &=
E\left[R(\tau)\left(\sum_{t=0}^{T-1}\nabla_\theta \ln\pi(a_t|s_t)\right)\right]
\end{align*}

軌道の$n$番目のサンプルを$\tau^{(n)}=(s_0^{(n)},a_0^{(n)},s_1^{(n)},a_1^{(n)},\cdots,s_T^{(n)})$とすると，
\begin{align*}
\nabla_\theta E[R(\tau)]
 \approx
\frac{1}{N}\sum_{n=1}^N \left[
R(\tau^{(n)})
\left(\sum_{t=0}^{T-1}\nabla_\theta \ln\pi(a_t^{(n)}|s_t^{(n)})\right)
\right]
\end{align*}

\if0
\section{Reward-to-go方策勾配}
Reward-to-goを次のように定義する．
\begin{align*}
R_t = \sum_{\ell=t}^{T-1}r(s_t,a_t)
\end{align*}
\fi
