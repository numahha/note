\section{GAE: Generalized Advantage Function \cite{schulman2016high}}

\subsection{準備}
初期状態$s_0$は，分布$\rho_0$からサンプルされる．
軌道$(s_0,a_0,s_1,a_1,\cdots)$は，終端状態にたどり着くまで，方策$a_t\sim \pi(a_t|s_t)$に従って行動をサンプリングし，またダイナミクス$s_{t+1}\sim P(s_{t+1}|s_t,a_t)$に従って状態をサンプリングすることにより生成される．
報酬$r_t=r(s_t,a_t,s_{t+1})$は，各時間ステップにおいて受け取る．
目標は，期待総報酬$\sum_{t=0}^{\infty}r_t$を最大化すことであり，全ての方策に対して有限であると仮定する．
なお，問題設定の一部として割引を定義しない．
以下において，バイアス-バリアンス（分散）トレードオフを調整するアルゴリズムパラメータとして，割引率が現れる．


方策個賠法は，勾配$g:=\nabla_\theta\mathbb{E}[\sum_{t=0}^{\infty}r_t]$を繰り返し推定することにより，期待総報酬を最大化する．
方策勾配に対する複数の関連する表現が存在する．
\begin{align*}
  g = \mathbb{E}_{s_{0:\infty},~a_{0:\infty}}\left[ \sum_{t=0}^{\infty}\Psi_t\nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \right]
\end{align*}
ここで，$\Psi_t$は，次のものがあり得る．
\begin{itemize}
  \item[1.] $\sum_{\ell=0}^{\infty}r_{\ell}$：初期時刻からの軌道の総報酬．
  \item[2.] $\sum_{\ell=0}^{\infty}r_{t+\ell}$：時刻$t$において$(s_t,a_t)$から始まる軌道の総報酬．
  \item[3.] $\sum_{\ell=0}^{\infty}r_{t+\ell}-b(s_t)$：上からベースラインを引いたもの．
  \item[4.] $Q^{\pi}(s_t,a_t)$：Q値．
  \item[5.] $A^{\pi}(s_t,a_t):=Q^{\pi}(s_t,a_t)-V^{\pi}(s_t)$：アドバンテージ関数．
  \item[6.] $r_t + V^{\pi}(s_{t+1})-V^{\pi}(s_t)$：TD残差．
\end{itemize}
ここで，価値関数は次のように定義される．
\begin{align*}
  V^{\pi}(s_t) := \mathbb{E}_{s_{t+1:\infty},~a_{t:\infty}}\left[ \sum_{\ell=0}^{\infty}r_{t+\ell} \right],~~~~
  Q^{\pi}(s_t,a_t):= \mathbb{E}_{s_{t+1:\infty},~a_{t+1:\infty}}\left[ \sum_{\ell=0}^{\infty}r_{t+\ell} \right]
\end{align*}
$\Psi_t=A^{\pi}(s_t,a_t)$とすることにより分散をほぼ最小にすることができるが，実際にはアドバンテージ関数は未知で推定されなければならない．

パラメータ$\gamma$を導入する．
これにより，
バイアスを導入してしまうことを引き換えにして，
遠い報酬の重みを小さくすることにより分散を低くすることができる．
（注意：我々の言葉で言うとすれば，「割引を大きくするとJ値は小さくなり，またその結果としてサンプル推定の分散も小さくなるが，元々解きたい問題からどんどん離れていく」となる）
このパラメータは，MDPの割引定式において用いられる割引率に対応するが，この論文では割引問題における分散低減パラメータとして扱う．
割引価値関数は，次により与えられる．
\begin{align*}
  V^{\pi,\gamma}(s_t) := \mathbb{E}_{s_{t+1:\infty},~a_{t:\infty}}\left[ \sum_{\ell=0}^{\infty}\gamma^{\ell}r_{t+\ell} \right],~~~~
  Q^{\pi,\gamma}(s_t,a_t):= \mathbb{E}_{s_{t+1:\infty},~a_{t+1:\infty}}\left[ \sum_{\ell=0}^{\infty}\gamma^{\ell}r_{t+\ell} \right]
\end{align*}
また，割引アドバンテージ関数$A^{\pi,\gamma}(s_t,a_t):=Q^{\pi,\gamma}(s_t,a_t)-V^{\pi,\gamma}(s_t)$とする．
方策勾配の割引近似は，次のように定義される．
\begin{align*}
  g^{\gamma} = \mathbb{E}_{s_{0:\infty},~a_{0:\infty}}\left[ \sum_{t=0}^{\infty}A^{\pi,\gamma}(s_t,a_t)\nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \right]
\end{align*}
以下では，バイアスされた（しかしバイアスされすぎてはいない）$A^{\pi,\gamma}$に対する推定をどのように得るかを議論する．


\subsection{アドバンテージ関数推定}
関数$V$のTD残差を，$\delta_t := -V(s_t) + [ r_t + \gamma V(s_{t+1}) ]$で定義する．
もし価値関数が$V=V^{\pi,\gamma}$という条件を満たす（この論文では$\gamma$-justと呼ぶ．割引問題において方策勾配がバイアスしない／不偏であることの条件）なら，
\begin{align*}
  \mathbb{E}_{s_{t+1}}\left[ \delta_t\right]
  &=
  -V^{\pi,\gamma}(s_t) + \mathbb{E}_{s_{t+1}}\left[ r(s_t,a_t,s_{t+1}) + \gamma V^{\pi,\gamma}(s_{t+1})\right]
  =
  - V^{\pi,\gamma}(s_t) + Q^{\pi,\gamma}(s_t,a_t)
  =
  A^{\pi,\gamma}(s_t,a_t)
\end{align*}
となる．よって，もし$V=V^{\pi,\gamma}$なら（このときのみであるが），$\delta_t$はアドバンテージ関数の不偏推定量である．
$\delta_t$の$k$個の割引和は，次のように書ける．
\begin{align*}
  \hat{A}_t^{(k)} := \sum_{\ell=0}^{k-1}\gamma^\ell \delta_t
  =
  - V(s_t) + \left[\sum_{\ell=0}^{k-1}\gamma^\ell r_t + \gamma^k V(s_{t+k})\right]
\end{align*}
この式は，$k$ステップリターンの推定からベースライン$-V(s_t)$を引いたものである．
$\delta_t=\hat{A}_t^{(1)}$との類似で，$\hat{A}_t^{(k)}$をアドバンテージ関数の推定として考えよう．
ただし，$k\to\infty$とするにつれて，バイアスは大きくなる．
なお，$k\to\infty$では，$\hat{A}_t^{(\infty)}=-V(s_t)+\sum_{\ell=0}^{\infty}\gamma^\ell r_t$となり，経験リターンから価値関数ベースラインを引いたものが得られる．

一般化アドバンテージ関数(GAE)を，次のように定義する．
\begin{align}
  \hat{A}_t^{GAE(\gamma,\lambda)}
  := (1-\lambda)\sum_{k=0}^{\infty}\lambda^k\hat{A}_t^{(k)}
  = \sum_{t=0}^{\infty}(\gamma\lambda)^{\ell}\delta_{t+\ell}
  \label{eq:gae}
\end{align}
これは，価値関数の推定のためのTD($\lambda$)を，アドバンテージ関数に対応させたものである．
$\lambda=0,~1$の時には，次が得られる．
\begin{align*}
  \hat{A}_t^{GAE(\gamma,0)}
  &= - V(s_t) + r_t + \gamma V(s_{t+1})
  \\
  \hat{A}_t^{GAE(\gamma,1)}
  &= - V(s_t) + \sum_{\ell=0}^{\infty}\gamma^\ell r_t
\end{align*}
$\hat{A}_t^{GAE(\gamma,0)}$は，どのような$V$に対してもバイアスを引き起こさないが，分散が大きい．
$\hat{A}_t^{GAE(\gamma,1)}$は，$V=V^{\pi,\gamma}$の時のみバイアスせず，それ以外ではバイアスを引き起こすが，低い分散を持つ．



\subsection{価値関数近似}
価値関数を表すための非線形関数近似器を用いる場合，最も単純なアプローチは非線形回帰問題を解くことである．
\begin{align*}
  \min_{\phi}\sum_{n=1}^N ||V_\phi(s_n)-\hat{V}_n ||^2
\end{align*}
ここで，$\hat{V}_t=\sum_{\ell=0}^{\infty}\gamma^{\ell} r_{t+\ell}$は，報酬の割引総和であり，$n$は軌道のバッチにおける全ての時間ステップにわたる添え字である（ここでの$n$はバッチの添え字ではないことに注意！つまり，エピソードについての$t=1,\cdots,\infty$を縦に並べて順番付けしたものがここでの$n$である）．


論文\cite{schulman2016high}の実験では，バッチ最適化の各反復法において価値関数を，trust region法を用いて最適化する．
trust region法は，データの一番最近のバッチにオーバーフィッチングすることを避ける助けとなる．
trust region問題を定式化するために，
まず$\sigma^2=\frac{1}{N}\sum_{n=1}^N||V_{\phi_{old}}(s_n)-\hat{V}_n ||^2$を計算する．
ここで，$\phi_{old}$は，最適化前のパラメータである．
そうして，次の制約付き最適化問題を解く．
\begin{align}
  \begin{array}{ll}
    \min_{\phi} & \sum_{n=1}^N ||V_\phi(s_n)-\hat{V}_n ||^2 \\
    s.t. & \frac{1}{N} \sum_{n=1}^N \frac{||V_\phi(s_n)-\hat{V}_{\phi_{old}}(s_n) ||^2}{2\sigma^2}\le\epsilon
  \end{array}
  \label{eq:constraint_problem_for_value_approximation}
\end{align}
この拘束は，価値関数は平均$V_\phi(s)$で分散$\sigma^2$を持つ条件付きガウス分布をパラメトライズするように取るとした場合の，以前の価値関数と新しい価値関数の間の平均KLダイバージェンスが$\epsilon$よりも小さくなる拘束と等価である．

共役勾配アルゴリズムを用いてtrust region問題の近似解を計算する．
具体的には，次の2次計画問題を解く．
\begin{align}
  \begin{array}{ll}
    \min_{\phi} & g^T(\phi_-\phi_{old}) \\
    s.t. & (\phi_-\phi_{old})^T\left[\frac{1}{N}\sum_nj_nj_n^T\right]_{\phi_{old}}(\phi_-\phi_{old})\le\epsilon
  \end{array}
  \label{eq:approximate_constraint_problem_for_value_approximation}
\end{align}
ここで，
$g$は目的関数$\sum_{n=1}^N ||V_\phi(s_n)-\hat{V}_n ||^2$の勾配であり，
また$j_n=\nabla_\phi V_\phi(s_n)$である．
（式(\ref{eq:constraint_problem_for_value_approximation})から式(\ref{eq:approximate_constraint_problem_for_value_approximation})の導出は後述）




\subsection{方策最適化アルゴリズム}
GAEは様々な方策勾配法に対して用いることができるが，この論文ではTRPO\cite{schulman2015trust}を用いて方策を更新する．
TRPOは，各反復において次の制約付き最適化問題を解く．
\begin{align}
  \begin{array}{ll}
    \min_{\theta} & \frac{1}{N}\sum_{n=1}^{N}\frac{\pi_\theta(a_n|s_n)}{\pi_{\theta_{old}}(a_n|s_n)}\hat{A}_n\\
    s.t. & \frac{1}{N}\sum_{n=1}^N D_{KL}\left( \pi_{\theta_{old}}(\cdot|s_n) || \pi_{\theta}(\cdot|s_n) \right)\le\epsilon
  \end{array}
  \label{eq:trpo_with_gae}
\end{align}
TRPOの論文\cite{schulman2015trust}に示されているように，目的関数を線形化・制約を2次化することによりこの問題を近似的に解く．
これは，$\theta-\theta_{old}\propto -F^{-1}g$の方向のステップを得て，ここで$F$は平均Fisher情報行列，$g$は方策勾配推定である．
この方策更新は，自然方策勾配やnatural actor-criticと同じステップ方向を得るが，異なるステップサイズ決定スキームと計算手法を用いる．


方策と価値関数を反復的に更新する全体的なアルゴリズムは，Algorithm~\ref{alg:trpo_with_gae}に与えられる．
\begin{algorithm}[t]
   \caption{TRPO with GAE}
   \label{alg:trpo_with_gae}
   \begin{algorithmic}[1]
     \State 方策パラメータ$\theta_0$と価値観すパラメータ$\phi_0$を初期化する．
     \For{$i=0,1,\cdots$}
     \State $N$時間ステップが得られるまで，現在の方策$\pi_\theta$をシミュレートする．
     \State $V=V_\phi$を用いて，全ての時刻$t\in\{ 1,2,\cdots\}$における$\delta_t^V$を計算する．
     \State $\hat{A}_t=\sum_{\ell=0}^{\infty}(\gamma\lambda)^{\ell}\delta_{t+\ell}^V$を全ての時間ステップに対して計算する．
     \State 式(\ref{eq:trpo_with_gae})のTRPO更新により$\theta_{i+1}$を計算する．
     \State 式(\ref{eq:approximate_constraint_problem_for_value_approximation})により$\phi_{i+1}$を計算する．
     \EndFor
\end{algorithmic}
\end{algorithm}
方策更新$\theta_i\to\theta_{i+1}$は価値関数$V_{\phi_i}$を用いて行われて，$V_{\phi_{i+1}}$ではないことに注意する．
もし価値関数を先に更新すると，追加的なバイアスが導入されるだろう．
これを見るために，価値関数がオーバーフィットした極端な状況を考えると，Bellman残差$r_t+\gamma V(s_{t+1})-V(s_t)$は全ての時間ステップにおいてゼロになり，つまり方策勾配もゼロになる．



\subsection{自分のための補足}
\paragraph{KLダイバージェンス．}
連続確率変数の場合，一般に，KLダイバージェンスは次のように定義される．
\begin{align*}
KL(q(\vu)||p(\vu))
=
\int q(\vu)\ln\frac{q(\vu)}{p(\vu)}d\vu
=
\int q(\vu)[\ln q(\vu)-\ln p(\vu)]d\vu
\end{align*}
正規分布$q(u)=\mathcal{N}(\vu|\vmu_q,\vSigma_{q})$，$p(u)=\mathcal{N}(\vu|\vmu_p,\vSigma_{p})$とすると，
\begin{align*}
&KL(q(\vu)||p(\vu))
=
\int q(\vu)[\ln q(\vu)-\ln p(\vu)]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
+\int q(\vu)\left[-\frac{1}{2}(\vu-\vmu_q)^T\vSigma_q^{-1}(\vu-\vmu_q)-\frac{1}{2}(\vu-\vmu_p)^T\vSigma_p^{-1}(\vu-\vmu_p)\right]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-\frac{1}{2}\int q(\vu)\mbox{tr}\left[\vSigma_q^{-1}(\vu-\vmu_q)(\vu-\vmu_q)^T-\vSigma_p^{-1}(\vu-\vmu_p)(\vu-\vmu_p)^T\right]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-\frac{1}{2}\mbox{tr}[\vSigma_q^{-1}\vSigma_q]
+\frac{1}{2}\int q(\vu)\mbox{tr}\left[\vSigma_p^{-1}(\vu\vu^T-2\vmu_p\vu^T+\vmu_p\vmu_p^T)\right]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-\frac{D}{2}
+\frac{1}{2}\mbox{tr}\int q(\vu)\left[\vSigma_p^{-1}\vu\vu^T\right]d\vu
+\frac{1}{2}\mbox{tr}\left[\vSigma_p^{-1}(-2\vmu_p\vmu_q^T+\vmu_p\vmu_p^T)\right]d\vu
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}(-2\vmu_p\vmu_q^T+\vmu_p\vmu_p^T)\right]
+\int q(\vu)\mbox{tr}\left[\vSigma_p^{-1}[(\vu-\vmu_q)(\vu-\vmu_q)^T + 2\vmu_q\vu^T - \vmu_q\vmu_q^T]\right]d\vu
\right\}
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}(-2\vmu_p\vmu_q^T+\vmu_p\vmu_p^T)\right]
+\mbox{tr}\left[\vSigma_p^{-1}(\vSigma_q + 2\vmu_q\vmu_q^T - \vmu_q\vmu_q^T)\right]
\right\}
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}(\vSigma_q  + \vmu_p\vmu_p^T -2\vmu_p\vmu_q^T+ \vmu_q\vmu_q^T)\right]
\right\}
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}\vSigma_q\right]
+(\vmu_p-\vmu_q)^T\vSigma_p^{-1}(\vmu_p-\vmu_q)
\right\}
\end{align*}
（注：トレースの性質$\mbox{tr}(X+Y)=\mbox{tr}(X)+\mbox{tr}(Y)$と$\mbox{tr}(XY)=\mbox{tr}(YX$を使った）
（よく使う式で，例えばノート\cite{duchi2007derivations}の最後の節に導出が乗っている）

1変数の場合は，$q(u)={\mathcal{N}}(u|\mu_q,\sigma^2_q)$と$p(u)={\mathcal{N}}(u|\mu_p,\sigma^2_p)$として，
\begin{align*}
  D_{KL}(q(u)||p(u))
  &= \int_{-\infty}^{\infty}q(u)\ln\frac{q(u)}{p(u)}du
  \\
  &= \int_{-\infty}^{\infty}q(u)\left(
  -\frac{(u-\mu_q)^2}{2\sigma_q^2}
  -\frac{1}{2}\ln(2\pi\sigma_q^2)
  +\frac{(u-\mu_p)^2}{2\sigma_p^2}
  +\frac{1}{2}\ln(2\pi\sigma_p^2)
   \right)du
  \\
  &=
  -
  \left( \frac{(\mu^2_q+\sigma_q^2) -2\mu_q^2 + \mu_q^2}{2\sigma_q^2} \right)
  +
  \left( \frac{(\mu^2_q+\sigma_q^2) -2\mu_q\mu_p + \mu_p^2}{2\sigma_p^2}\right)
  +
  \frac{1}{2}\ln \frac{\sigma_p^2}{\sigma_q^2}
  \\
  &=
  -
  \frac{1}{2}
  +
  \left(\frac{\sigma_q^2+(\mu_q-\mu_p)^2}{2\sigma_p^2}\right)
  +
  \frac{1}{2}\ln \frac{\sigma_p^2}{\sigma_q^2}
 \\
  &=
  \frac{1}{2}\left\{
  \ln \frac{\sigma_p^2}{\sigma_q^2}
  -
  1
  +
  \left(\frac{\sigma_q^2}{\sigma_p^2}\right)
  +
  \left(\frac{(\mu_p-\mu_q)^2}{\sigma_p^2}\right)
  \right\}
\end{align*}
平均$\mu_i$を更新することだけを考える場合には，KLダイバージェンスの中で$\frac{(\mu_q-\mu_p)^2}{2\sigma_p^2}$の項以外は関係ない．
式(\ref{eq:constraint_problem_for_value_approximation})は，更新するパラメータに関係ない項を$\epsilon$の中に押し込んで書いてある．


\paragraph{式(\ref{eq:constraint_problem_for_value_approximation})拘束条件のTaylor展開．}
$\lambda=1$としたアドバンテージ関数の推定は，論文\cite{schulman2016high}のEq.18より，$\hat{A}_t^{GAE(\gamma,1)}=\sum_{\ell=0}^{\infty}\gamma^{\ell}r_{t+\ell}-V(s_t)=\hat{V}_t-V(s_t)$と得られる．
ここで，$V(s)=V^{\pi,\gamma}(s)$として，現在の価値関数近似器$V_{\phi_{old}}(s)$を用いると，
\begin{align*}
  \sum_{n=1}^N||V_\phi(s_n)-\hat{V}_n ||^2
  =
  \sum_{n=1}^N||V_\phi(s_n)-\left[ \hat{A}_n^{GAE(\gamma,1)}+V_{\phi_{old}}(s_n)\right]||^2
\end{align*}
となる
（この論文内では$\lambda=1$の場合を実装したが，一般化して$\hat{V}_t^{\lambda}=\sum{\ell=0}^{\infty}(\gamma\lambda)^{\ell} r_{t+\ell}=\hat{A}_t^{GAE(\gamma,1)}+V_{\phi_{old}}(s_n)$でも良いとのこと）．



\subsection{まとめ}
\begin{itemize}
\item パラメトライズされた方策表現と，非線形価値関数近似表現を用いる．
\item TD残差を用いて，一般化アドバンテージ関数のサンプル近似が式(\ref{eq:gae})で得られる．
\item TD残差を用いて，非線形価値関数近似器を更新する（with trust region法）
\item Algorithm~\ref{alg:trpo_with_gae}．
\end{itemize}
