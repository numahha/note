\section{GAE: Generalized Advantage Functionの定義\cite{schulman2016high}}

\subsection{準備}
初期状態$s_0$は，分布$\rho_0$からサンプルされる．
軌道$(s_0,a_0,s_1,a_1,\cdots)$は，
確率的定常方策$a_t\sim \pi(a_t|s_t)$に従って行動をサンプリングし，またダイナミクス$s_{t+1}\sim P(s_{t+1}|s_t,a_t)$に従って状態をサンプリングすることにより生成される．
方策を特徴づけるパラメータを$\theta$とする．
報酬$r_t=r(s_t,a_t,s_{t+1})$は，各ステップにおいて受け取る．
目標は，期待総報酬$\sum_{t=0}^{\infty} r_t$の最大化であり，全ての方策に対して有限であると仮定する．

価値関数を次のように定義する．
\begin{align*}
  V^{\pi}(s_t) := \mathbb{E}_{s_{t+1:\infty},~a_{t:\infty}}\left[ \sum_{\ell=0}^{\infty}r_{t+\ell} \right],~~~~
  Q^{\pi}(s_t,a_t):= \mathbb{E}_{s_{t+1:\infty},~a_{t+1:\infty}}\left[ \sum_{\ell=0}^{\infty}r_{t+\ell} \right]
\end{align*}
また，アドバンテージ関数を次のように定義する．
\begin{align*}
  A^{\pi}(s_t,a_t):=Q^{\pi}(s_t,a_t)-V^{\pi}(s_t)
\end{align*}


方策勾配法は，勾配$g:=\nabla_\theta\mathbb{E}[\sum_{t=0}^{\infty}r_t]$を繰り返し推定することにより，期待総報酬を最大化する．
方策勾配に対する複数の関連する表現が存在する．
\begin{align*}
  g = \mathbb{E}_{s_{0:\infty},~a_{0:\infty}}\left[ \sum_{t=0}^{\infty}\Psi_t\nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \right]
\end{align*}
$\Psi_t=A^{\pi}(s_t,a_t)$とすることにより分散をほぼ最小にすることができる．
実際にはアドバンテージ関数は未知で推定されなければならないため，これを推定する方法を以下で議論する．

\subsection{割引関数}
パラメータ$\gamma$を導入する．
これにより，
バイアスを導入してしまうことを引き換えにして，
遠い報酬の重みを小さくすることにより分散を低くすることができる．
このパラメータは，MDPの割引定式化において用いられる割引率に対応するが，この論文では割引問題における分散低減パラメータとして扱う．
割引価値関数は，次により与えられる．
\begin{align*}
  V^{\pi,\gamma}(s_t) := \mathbb{E}_{s_{t+1:\infty},~a_{t:\infty}}\left[ \sum_{\ell=0}^{\infty}\gamma^{\ell}r_{t+\ell} \right],~~~~
  Q^{\pi,\gamma}(s_t,a_t):= \mathbb{E}_{s_{t+1:\infty},~a_{t+1:\infty}}\left[ \sum_{\ell=0}^{\infty}\gamma^{\ell}r_{t+\ell} \right]
\end{align*}
また，割引アドバンテージ関数$A^{\pi,\gamma}(s_t,a_t):=Q^{\pi,\gamma}(s_t,a_t)-V^{\pi,\gamma}(s_t)$とする．
方策勾配の割引近似は，次のように定義される．
\begin{align*}
  g^{\gamma} = \mathbb{E}_{s_{0:\infty},~a_{0:\infty}}\left[ \sum_{t=0}^{\infty}A^{\pi,\gamma}(s_t,a_t)\nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \right]
\end{align*}
以下では，$A^{\pi,\gamma}$に対する推定をどのように得るかを議論する．


\subsection{アドバンテージ関数}
関数$V$のTD誤差を，$\delta_t^V := -V(s_t) + [ r_t + \gamma V(s_{t+1}) ]$と定義する．
もし価値関数が$V=V^{\pi,\gamma}$という条件を満たすなら，次が成り立つ．
\begin{align*}
  \mathbb{E}_{s_{t+1}}\left[ \delta_t^V\right]
  &=
  -V^{\pi,\gamma}(s_t) + \mathbb{E}_{s_{t+1}}\left[ r(s_t,a_t,s_{t+1}) + \gamma V^{\pi,\gamma}(s_{t+1})\right]
  \\
  &=
  - V^{\pi,\gamma}(s_t) + Q^{\pi,\gamma}(s_t,a_t)
  \\
  &=
  A^{\pi,\gamma}(s_t,a_t)
\end{align*}
従って，もし$V=V^{\pi,\gamma}$なら，$\delta_t^V$はアドバンテージ関数の不偏推定量である．

$\delta_t^V$の$k$個の割引和は，次のように書ける．
\begin{align*}
 \hat{A}_t^{(1)}
 &:= \delta_{t}^V
 =
 -V(s_t) + [ r_t + \gamma V(s_{t+1}) ]
 \\
  \hat{A}_t^{(k)}
  &:= \sum_{\ell=0}^{k-1}\gamma^\ell \delta_{t+\ell}^V
  =
  - V(s_t) + \left[\sum_{\ell=0}^{k-1}\gamma^\ell r_{t+\ell} + \gamma^k V(s_{t+k})\right]
\end{align*}
この式は，$k$ステップリターンの推定からベースライン$-V(s_t)$を引いたものである．
$\delta_t=\hat{A}_t^{(1)}$との類似で，$\hat{A}_t^{(k)}$をアドバンテージ関数の推定として考える．
$k\to\infty$とするにつれて，$\gamma^k V(s_{t+k})$の項が小さくなっていくので，バイアスは小さくなる．
なお，$k\to\infty$では，
\begin{align*}
\hat{A}_t^{(\infty)}=-V(s_t)+\sum_{\ell=0}^{\infty}\gamma^\ell r_t
\end{align*}
となり，経験リターン$\sum_{\ell=0}^{\infty}\gamma^\ell r_t$から価値関数ベースライン$V(s_t)$を引いたものが得られる．

\paragraph{一般化アドバンテージ関数．}
一般化アドバンテージ関数(GAE)を，$k$ステップ推定量の指数重み付け平均として次のように定義する．
\begin{align}
  \hat{A}_t^{GAE(\gamma,\lambda)}
  := (1-\lambda)\sum_{k=0}^{\infty}\lambda^k\hat{A}_t^{(k)}
  = \sum_{t=0}^{\infty}(\gamma\lambda)^{\ell}\delta_{t+\ell}^V
  \label{eq:gae}
\end{align}
これは，価値関数の推定のためのTD($\lambda$)を，アドバンテージ関数に対応させたものである．
$\lambda=0,~1$の時には，次が得られる．
\begin{align*}
  \hat{A}_t^{GAE(\gamma,0)}
  &= \hat{A}_t^{(0)}
  =  - V(s_t) + r_t + \gamma V(s_{t+1})
  \\
  \hat{A}_t^{GAE(\gamma,1)}
  &= \hat{A}_t^{(\infty)}
  = - V(s_t) + \sum_{\ell=0}^{\infty}\gamma^\ell r_t
\end{align*}

\paragraph{注意．}
\begin{itemize}
\item $\gamma$は，割引関数を定義するために導入された量である．
\item $\lambda$は，推定のために追加で導入された量である．
\end{itemize}



\clearpage
\section{GAEの使い方}
オリジナルの論文\cite{schulman2016high}ではTRPO \cite{schulman2015trust}と組み合わせるやり方で書いてあるけど，
ここでは特定の勾配法に拘らないやり方で書く．
次のように近似器を置く．
\begin{itemize}
\item $V_\phi$: 価値関数の近似器
\item $\pi_\theta$: 方策の近似器．
\end{itemize}

方策と価値関数を反復的に更新する全体的なアルゴリズムは，Algorithm~\ref{alg:trpo_with_gae}に与えられる．
\begin{algorithm}[t]
   \caption{PG with GAE}
   \label{alg:trpo_with_gae}
   \begin{algorithmic}[1]
     \State 方策パラメータ$\theta_0$と価値価値関数パラメータ$\phi_0$を初期化する．
     \For{$i=0,1,\cdots$}
     \State $N$時間ステップが得られるまで，現在の方策$\pi_\theta$をシミュレートする．
     \State $V=V_\phi$を用いて，全ての時刻$t\in\{ 1,2,\cdots\}$における$\delta_t^V$を計算する．
     \State $\hat{A}_t=\sum_{\ell=0}^{\infty}(\gamma\lambda)^{\ell}\delta_{t+\ell}^V$を全ての時間ステップに対して計算する．
     \State 方策更新：$\theta_{i+1}$を計算する．式(\ref{eq:pg})や(\ref{eq:trpo_with_gae})．
     \State 価値関数近似更新：$\phi_{i+1}$を計算する．式(\ref{eq:approximate_constraint_problem_for_value_approximation})．
     \EndFor
\end{algorithmic}
\end{algorithm}
方策更新$\theta_i\to\theta_{i+1}$は価値関数$V_{\phi_i}$を用いて行われて，$V_{\phi_{i+1}}$ではないことに注意する．
もし$V_{\phi_{i+1}}$を用いて方策更新してしまうと，次のような不具合が起こる：
価値関数がオーバーフィットした状況だとを考えると，Bellman残差$r_t+\gamma V(s_{t+1})-V(s_t)$は全ての時間ステップにおいてゼロになり，したがって方策勾配もゼロになり，方策は改善されない．


\subsection{アドバンテージ関数推定を用いた方策勾配}
状態遷移と報酬の観測系列$\{ s_t,a_t\}_{t=0}^N$と$\{ r_t \}_{t=0}^N$が与えられた場合，現在の価値関数近似$V_\phi$を用いて，アドバンテージ関数を次のように推定する．
\begin{align*}
  \hat{A}_t^{GAE(\gamma,\lambda)}
  = \sum_{t=0}^{N}(\gamma\lambda)^{\ell}\delta_{t+\ell}^{V_{\phi}}
  = \sum_{t=0}^{N}(\gamma\lambda)^{\ell}\left[-V_\phi(s_{t+\ell}) + r_{t+\ell} + \gamma V_\phi(s_{t+1+\ell})
  \right]
\end{align*}
\paragraph{通常の方策勾配．}
通常の方策勾配法では，
$\min_\theta \mathbb{E}[\sum_{t=0}^{\infty}r_t]$を解くために，各反復で勾配$\nabla_\theta\mathbb{E}[\sum_{t=0}^{\infty}r_t]$を考える．
方策勾配の割引近似を，次のように計算する．
\begin{align}
  g^{\gamma}
   &=
   \mathbb{E}_{s_{0:\infty},~a_{0:\infty}}\left[ \sum_{t=0}^{\infty}A^{\pi,\gamma}(s_t,a_t)\nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \right]
   \notag
   \\
   &\approx
   \sum_{t=0}^N \hat{A}^{GAE(\pi,\gamma)}_t\nabla_{\theta}\log\pi_{\theta}(a_t|s_t)
   \label{eq:pg}
\end{align}

\paragraph{TRPO．}
雑に言うと，TRPOの場合，各反復での更新量を制限するために，次の問題を解くことを考える．
\begin{align*}
  \begin{array}{ll}
    \min_{\theta} & \mathbb{E}[\sum_{t=0}^{\infty}r_t]\\
    \mbox{s.t.}   & ||\theta-\theta_{old}||\le\epsilon
  \end{array}
\end{align*}
ちゃんと書くと次のようになる（\cite{schulman2016high}や\cite{schulman2015trust}を参照）．
\begin{align}
  \begin{array}{ll}
    \min_{\theta} & \frac{1}{N}\sum_{n=1}^{N}\frac{\pi_\theta(a_n|s_n)}{\pi_{\theta_{old}}(a_n|s_n)}\hat{A}_n\\
    s.t. & \frac{1}{N}\sum_{n=1}^N D_{KL}\left( \pi_{\theta_{old}}(\cdot|s_n) || \pi_{\theta}(\cdot|s_n) \right)\le\epsilon
  \end{array}
  \label{eq:trpo_with_gae}
\end{align}


\subsection{価値関数推定}
サンプルされた報酬の割引総和を，次のようにおく．
\begin{align*}
\hat{V}_t=\sum_{\ell=0}^{N-t}(\lambda\gamma)^{\ell} r_{t+\ell}
\end{align*}
価値関数を非線形関数近似する場合，最も単純なアプローチは非線形回帰問題を解くことである．
\begin{align*}
  \min_{\phi}\sum_{n=1}^N ||V_\phi(s_n)-\hat{V}_n ||^2
\end{align*}

論文\cite{schulman2016high}では，各反復法において価値関数を，信頼領域法\cite{nocedal2006numerical}を用いて最適化する
（これは，価値関数推定の非線形最適化問題に対して信頼領域法を使っているのであり，方策最適化に対して信頼領域法を使うTRPOとは違うという事に注意！）
信頼領域問題を定式化するために，
まず$\sigma^2=\frac{1}{N}\sum_{n=1}^N||V_{\phi_{old}}(s_n)-\hat{V}_n ||^2$を計算する．
ここで，$\phi_{old}$は，最適化前のパラメータである．
そうして，次の制約付き最適化問題を解く．
\begin{align}
  \begin{array}{ll}
    \min_{\phi} & \sum_{n=1}^N ||V_\phi(s_n)-\hat{V}_n ||^2 \\
    s.t. & \frac{1}{N} \sum_{n=1}^N \frac{||V_\phi(s_n)-V_{\phi_{old}}(s_n) ||^2}{2\sigma^2}\le\epsilon
  \end{array}
  \label{eq:constraint_problem_for_value_approximation}
\end{align}
この拘束は，価値関数は平均$V_\phi(s)$で分散$\sigma^2$を持つ条件付きガウス分布をパラメトライズするように取るとした場合の，以前の価値関数と新しい価値関数の間の平均KLダイバージェンスが$\epsilon$よりも小さくなる拘束と等価である．

次の2次計画問題を解ことにより，信頼領域問題の近似解を計算する．
\begin{align}
  \begin{array}{ll}
    \min_{\phi} & g^T(\phi_-\phi_{old}) \\
    s.t. & (\phi_-\phi_{old})^T\left[\frac{1}{N}\sum_nj_nj_n^T\right]_{\phi_{old}}(\phi_-\phi_{old})\le\epsilon
  \end{array}
  \label{eq:approximate_constraint_problem_for_value_approximation}
\end{align}
ここで，
$g$は目的関数$\sum_{n=1}^N ||V_\phi(s_n)-\hat{V}_n ||^2$の勾配であり，
また$j_n=\nabla_\phi V_\phi(s_n)$である．
%（式(\ref{eq:constraint_problem_for_value_approximation})から式(\ref{eq:approximate_constraint_problem_for_value_approximation})の導出は後述）


\clearpage
\section{自分のための補足}
\paragraph{KLダイバージェンス．}
連続確率変数の場合，一般に，KLダイバージェンスは次のように定義される．
\begin{align*}
KL(q(\vu)||p(\vu))
=
\int q(\vu)\ln\frac{q(\vu)}{p(\vu)}d\vu
=
\int q(\vu)[\ln q(\vu)-\ln p(\vu)]d\vu
\end{align*}
正規分布$q(u)=\mathcal{N}(\vu|\vmu_q,\vSigma_{q})$，$p(u)=\mathcal{N}(\vu|\vmu_p,\vSigma_{p})$とすると，
\begin{align*}
&KL(q(\vu)||p(\vu))
=
\int q(\vu)[\ln q(\vu)-\ln p(\vu)]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
+\int q(\vu)\left[-\frac{1}{2}(\vu-\vmu_q)^T\vSigma_q^{-1}(\vu-\vmu_q)-\frac{1}{2}(\vu-\vmu_p)^T\vSigma_p^{-1}(\vu-\vmu_p)\right]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-\frac{1}{2}\int q(\vu)\mbox{tr}\left[\vSigma_q^{-1}(\vu-\vmu_q)(\vu-\vmu_q)^T-\vSigma_p^{-1}(\vu-\vmu_p)(\vu-\vmu_p)^T\right]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-\frac{1}{2}\mbox{tr}[\vSigma_q^{-1}\vSigma_q]
+\frac{1}{2}\int q(\vu)\mbox{tr}\left[\vSigma_p^{-1}(\vu\vu^T-2\vmu_p\vu^T+\vmu_p\vmu_p^T)\right]d\vu
\\
&=
\frac{1}{2}\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-\frac{D}{2}
+\frac{1}{2}\mbox{tr}\int q(\vu)\left[\vSigma_p^{-1}\vu\vu^T\right]d\vu
+\frac{1}{2}\mbox{tr}\left[\vSigma_p^{-1}(-2\vmu_p\vmu_q^T+\vmu_p\vmu_p^T)\right]d\vu
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}(-2\vmu_p\vmu_q^T+\vmu_p\vmu_p^T)\right]
+\int q(\vu)\mbox{tr}\left[\vSigma_p^{-1}[(\vu-\vmu_q)(\vu-\vmu_q)^T + 2\vmu_q\vu^T - \vmu_q\vmu_q^T]\right]d\vu
\right\}
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}(-2\vmu_p\vmu_q^T+\vmu_p\vmu_p^T)\right]
+\mbox{tr}\left[\vSigma_p^{-1}(\vSigma_q + 2\vmu_q\vmu_q^T - \vmu_q\vmu_q^T)\right]
\right\}
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}(\vSigma_q  + \vmu_p\vmu_p^T -2\vmu_p\vmu_q^T+ \vmu_q\vmu_q^T)\right]
\right\}
\\
&=
\frac{1}{2}\left\{
\ln \frac{\mbox{det}\vSigma_p}{\mbox{det}\vSigma_q}
-D
+\mbox{tr}\left[\vSigma_p^{-1}\vSigma_q\right]
+(\vmu_p-\vmu_q)^T\vSigma_p^{-1}(\vmu_p-\vmu_q)
\right\}
\end{align*}
（注：トレースの性質$\mbox{tr}(X+Y)=\mbox{tr}(X)+\mbox{tr}(Y)$と$\mbox{tr}(XY)=\mbox{tr}(YX$を使った）
（よく使う式で，例えばノート\cite{duchi2007derivations}の最後の節に導出が乗っている）

1変数の場合は，$q(u)={\mathcal{N}}(u|\mu_q,\sigma^2_q)$と$p(u)={\mathcal{N}}(u|\mu_p,\sigma^2_p)$として，
\begin{align*}
  D_{KL}(q(u)||p(u))
  &= \int_{-\infty}^{\infty}q(u)\ln\frac{q(u)}{p(u)}du
  \\
  &= \int_{-\infty}^{\infty}q(u)\left(
  -\frac{(u-\mu_q)^2}{2\sigma_q^2}
  -\frac{1}{2}\ln(2\pi\sigma_q^2)
  +\frac{(u-\mu_p)^2}{2\sigma_p^2}
  +\frac{1}{2}\ln(2\pi\sigma_p^2)
   \right)du
  \\
  &=
  -
  \left( \frac{(\mu^2_q+\sigma_q^2) -2\mu_q^2 + \mu_q^2}{2\sigma_q^2} \right)
  +
  \left( \frac{(\mu^2_q+\sigma_q^2) -2\mu_q\mu_p + \mu_p^2}{2\sigma_p^2}\right)
  +
  \frac{1}{2}\ln \frac{\sigma_p^2}{\sigma_q^2}
  \\
  &=
  -
  \frac{1}{2}
  +
  \left(\frac{\sigma_q^2+(\mu_q-\mu_p)^2}{2\sigma_p^2}\right)
  +
  \frac{1}{2}\ln \frac{\sigma_p^2}{\sigma_q^2}
 \\
  &=
  \frac{1}{2}\left\{
  \ln \frac{\sigma_p^2}{\sigma_q^2}
  -
  1
  +
  \left(\frac{\sigma_q^2}{\sigma_p^2}\right)
  +
  \left(\frac{(\mu_p-\mu_q)^2}{\sigma_p^2}\right)
  \right\}
\end{align*}
平均$\mu_i$を更新することだけを考える場合には，KLダイバージェンスの中で$\frac{(\mu_q-\mu_p)^2}{2\sigma_p^2}$の項以外は関係ない．
式(\ref{eq:constraint_problem_for_value_approximation})は，更新するパラメータに関係ない項を$\epsilon$の中に押し込んで書いてある．

\if0
\paragraph{式(\ref{eq:constraint_problem_for_value_approximation})拘束条件のTaylor展開．}
$\lambda=1$としたアドバンテージ関数の推定は，論文\cite{schulman2016high}のEq.18より，$\hat{A}_t^{GAE(\gamma,1)}=\sum_{\ell=0}^{\infty}\gamma^{\ell}r_{t+\ell}-V(s_t)=\hat{V}_t-V(s_t)$と得られる．
ここで，$V(s)=V^{\pi,\gamma}(s)$として，現在の価値関数近似器$V_{\phi_{old}}(s)$を用いると，
\begin{align*}
  \sum_{n=1}^N||V_\phi(s_n)-\hat{V}_n ||^2
  =
  \sum_{n=1}^N||V_\phi(s_n)-\left[ \hat{A}_n^{GAE(\gamma,1)}+V_{\phi_{old}}(s_n)\right]||^2
\end{align*}
となる
（この論文内では$\lambda=1$の場合を実装したが，
一般化して$\hat{V}_t^{\lambda}=\sum_{\ell=0}^{\infty}(\gamma\lambda)^{\ell} r_{t+\ell}=\hat{A}_t^{GAE(\gamma,1)}+V_{\phi_{old}}(s_n)$でも良いとのこと）．
\fi
